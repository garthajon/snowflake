{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6o7cmrw4txzwyqxvfi2v",
   "authorId": "1636540973752",
   "authorName": "GARTHJON",
   "authorEmail": "garth.jones@nhs.net",
   "sessionId": "33c68063-9fb8-4ded-9377-c999682b0e3f",
   "lastEditTime": 1747071442235
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_packages",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "#Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.functions import udf\nimport snowflake.snowpark.functions as F\n\nimport numpy as np\n#Override np.float_ with np.float64\nnp.float_ = np.float64\n\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd  \n# need to add numpy code from previous notebook to handle int64 issue for numpy\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error \n\n# other libs \nimport json\nimport joblib \nimport cachetools \n\n# warning suppression \nimport warnings; warnings.simplefilter('ignore')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8532af82-f7cc-41ef-b9d4-bb0db8153f8a",
   "metadata": {
    "language": "python",
    "name": "establish_connection"
   },
   "outputs": [],
   "source": "# Get active session (current snowflake session)\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# add a query tag to the session \nsession.query_tag = {\"origin\":\"sf_sit-is\",\"name\":\"e2e_ml_snowparkpython\", \"version\":{\"major\":1,\"minor\":0}}\n\n# Set session context\nsession.use_role(\"ACCOUNTADMIN\")\n\n# get current solution prefix from warehouse name\nsolution_prefix = session.get_current_warehouse()\n#.strip(\"_\").split(\"_DS_WH\")[0]\n\n# Get the current role, warehouse, and database/schema\nprint(f\"Current role: {session.get_current_role()} | Current warehouse: {session.get_current_warehouse()} | DB SCHEMA: {session.sql('select current_database(), current_schema()').collect()}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abd0ac20-f891-4b93-a3d7-0278db9a6c28",
   "metadata": {
    "language": "python",
    "name": "load_data"
   },
   "outputs": [],
   "source": "# Data Loading\n# note that by default this is a snowpark/snowflake data frame\ndiamonds_df = session.table('DIAMONDS')\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c08fffbb-2f8e-47f9-aa88-93bd40e18b79",
   "metadata": {
    "language": "python",
    "name": "strip_out_double_quotes_from_column"
   },
   "outputs": [],
   "source": "#strip double quotes from column names\n\n# Function to strip double quotes from column names\ndef strip_double_quotes_from_column_names(df):\n    new_columns = [col.replace('\"', '') for col in df.columns]\n    return df.to_df(*new_columns)\n\n# Apply the function to the DataFrame\ndiamonds_df = strip_double_quotes_from_column_names(diamonds_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ff7d250-4518-47b7-9c42-ad6aea28ae03",
   "metadata": {
    "language": "python",
    "name": "Categorical_Columns"
   },
   "outputs": [],
   "source": "# Categorize all the features for processing\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To store the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_\", \"X\", \"Y\", \"Z\"]\n\nLABEL_COLUMNS = ['PRICE']\nOUTPUT_COLUMNS = ['PREDICTED_PRICE']\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cb3608c-66e7-4e7b-9b07-057530f57454",
   "metadata": {
    "language": "python",
    "name": "zzz_load_model_from_registry"
   },
   "outputs": [],
   "source": "# load the preprocessing model which alreadt exists in the model registry\n# model_registry = Registry(session, database_name=\"DATASCIENCE\", schema_name=\"PUBLIC\")\n# preprocessing_pipeline = model_registry.get_model('pre_process_diamond')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2398f85a-0c5c-4e05-bc6b-a4f68b71102a",
   "metadata": {
    "language": "python",
    "name": "load_preprocess_pipeline_from_models_stage"
   },
   "outputs": [],
   "source": "#session.use_database(f\"{solution_prefix}_PROD\")\n#session.use_schema(\"ANALYTICS\")\ndb=session.get_current_database()\n#.strip('')\n\n# Construct the file path using the solution_prefix\nfile_path = f\"@{db}.PUBLIC.models/preprocessing_pipeline.joblib.gz\"\n\nsession.file.get(file_path, '/tmp')\nPIPELINE_FILE = \"/tmp/preprocessing_pipeline.joblib.gz\"\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0bac32f-4b91-4d73-97b8-f357c9de0fe1",
   "metadata": {
    "language": "python",
    "name": "prepare_train_test_dataframe"
   },
   "outputs": [],
   "source": "\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\n# apply the preprocessing pipeline to the training and test data frames\n#train_df = preprocessing_pipeline.transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "800236b6-9670-4aac-a942-916414ffb216",
   "metadata": {
    "language": "python",
    "name": "build_the_regression_model_from_XGBoost_Library"
   },
   "outputs": [],
   "source": "# create the model - a regression ML model from the XGBoost ML library \nregressor = XGBRegressor(\n    input_cols=CATEGORICAL_COLUMNS_OE + NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n#train the model\nregressor.fit(train_df)\n#do the prediction with the model and put the prediction into a snowpark dataframe\nresult = regressor.predict(test_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e89ffac1-2497-454e-9b72-97df2e7e72c2",
   "metadata": {
    "language": "python",
    "name": "prediction_to_pandas_df"
   },
   "outputs": [],
   "source": "# using the same model do another prediction into a pandas dataframe\nregressor.predict(test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].to_pandas())\n\n# Display a smiley face upon successful execution\nfrom IPython.display import display, HTML\ndisplay(HTML('<h1>ðŸ˜Š</h1>'))\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1818d6f4-5654-49d2-b531-6b9232145f62",
   "metadata": {
    "language": "python",
    "name": "mean_absolute_percentage_error"
   },
   "outputs": [],
   "source": "mape = mean_absolute_percentage_error(df=result,\n                                      y_true_col_names=\"PRICE\",\n                                      y_pred_col_names=\"PREDICTED_PRICE\")\n\nresult.select(\"PRICE\", \"PREDICTED_PRICE\")\nprint(f\"Mean absolute percentage error: {mape}\")\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "381c0108-387e-440e-af42-86cbdebf9b65",
   "metadata": {
    "language": "python",
    "name": "scatter_graph_actuals_vs_predicted_with_trend_line"
   },
   "outputs": [],
   "source": "# Plot actual price vs predicted price\ng = sns.relplot(data=result[\"PRICE\",\"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\",y=\"PREDICTED_PRICE\", kind=\"scatter\")\n#add a red trend line\ng.ax.axline((0,0), slope=1, color=\"r\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc15542b-4a3d-40d1-9c3c-0a5142022010",
   "metadata": {
    "language": "python",
    "name": "hyperparameter_model_tuning"
   },
   "outputs": [],
   "source": "# hyperparameters are parameters which control how an ML model learns from the data for example\n# the kind of distance measurement to be used in a k-nearest neighbour model\n# parameters regularly are output from an ML model but in this case hyperparameters control how\n# the ml model learns. Since hyperparameters dictate how the model learns, it is a good idea\n# to find what the best hyperparameters are so as to ensure your model learns in the best way\n# GridSearchCV is a tool that can be used to find the best hyperparameter settings for your model\n# see: https://datagy.io/sklearn-gridsearchcv/#:~:text=In%20this%20tutorial%2C%20youâ€™ll%20learn%20how%20to%20use,for%20the%20best%20model%20is%20Scikit-Learnâ€™s%20GridSearchCV%20class.\n#One way to tune your hyper-parameters is to use a grid search. This is probably the simplest method as well as the most crude. In a grid search, you try a \n#grid of hyper-parameters and evaluate the performance of each combination of hyper-parameters.\n\n#estimator= takes an estimator object, such as a classifier or a regression model.\n#param_grid= takes a dictionary or a list of dictionaries. The dictionaries should be key-value pairs, where the key is the hyper-parameter and the value are the cases of hyper-parameter values to test.\n#cv= takes an integer that determines the cross-validation strategy to apply. If None is passed, then 5 is used.\n#scoring= takes a string or a callable. This represents the strategy to evaluate the performance of the test set.\n#n_jobs= represents the number of jobs to run in parallel. Since this is a time-consuming process, running more jobs in parallel (if your computer can handle it) can speed up the process.\n\n# tested this 10/05/2025 started at 13:32 - finished around 13:33 (so very fast actually)\n\n# use GridSearchCV\n\ngrid_search = GridSearchCV(\n    estimator=XGBRegressor(),\n    param_grid={\n        \"n_estimators\": [100, 200, 300, 400, 500, 600],\n        \"learning_rate\": [0.1, 0.2, 0.3, 0.4, 0.5],\n    },\n    n_jobs=5,\n    scoring=\"neg_mean_absolute_percentage_error\",\n    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n\n# Train\ngrid_search.fit(train_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fee892a8-5a84-4063-985b-816abbdc71b4",
   "metadata": {
    "language": "python",
    "name": "display_best_hyperparameters"
   },
   "outputs": [],
   "source": "print(grid_search.to_sklearn().best_estimator_)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5327984e-29b4-42f5-895f-2b0f9897a939",
   "metadata": {
    "language": "python",
    "name": "plot_gridsearch_results"
   },
   "outputs": [],
   "source": "# using the mean average percentage error score (MAPE) to guage the percentage of error\n# which result from the hyperparameters 'learning_rate' and 'n_estimators' we already know\n# from selecting the model with the best parameters that the best combination of these two\n#hyperparameters is a learning_rate of 4 combined with n_estimator of 500, but we can now\n# see this below visually\n\ngs_results = grid_search.to_sklearn().cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e65724af-ffd6-4ab7-b175-6f8eaa19ba3e",
   "metadata": {
    "language": "python",
    "name": "use_grid_search_result_for_better_prediction"
   },
   "outputs": [],
   "source": "result = grid_search.predict(test_df)\nresult = result.to_pandas()\n#result.select(\"PRICE\", \"PREDICTED_PRICE\")\nresult = result[[\"PRICE\", \"PREDICTED_PRICE\"]]\n#result\n#result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\nmape = mean_absolute_percentage_error(df=result,\n                                      y_true_col_names=\"PRICE\",\n                                      y_pred_col_names=\"PREDICTED_PRICE\")\n#result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n#print(f\"Mean absolute percentage error: {mape}\")\n",
   "execution_count": null
  }
 ]
}